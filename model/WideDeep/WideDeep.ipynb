{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WideDeep(nn.Module):\n",
    "    def __init__(self, input_config, shared_emb_config=None, use_moving_statistics=True):\n",
    "        super(WideDeep, self).__init__()\n",
    "        \n",
    "        self._use_moving_statistics = use_moving_statistics\n",
    "        self._emb_dict = {}\n",
    "        self._seq_names = set()\n",
    "        self.input_config = input_config\n",
    "\n",
    "        # 构建嵌入字典\n",
    "        for feat_name, config in input_config.items():\n",
    "            input_category = config['category']\n",
    "            if input_category == 'embedding':\n",
    "                self._emb_dict[feat_name] = feat_name\n",
    "\n",
    "            if 'seq_name' in config:\n",
    "                seq_name = config['seq_name']\n",
    "                self._seq_names.add(seq_name)\n",
    "\n",
    "        # 处理共享嵌入\n",
    "        if shared_emb_config is not None:\n",
    "            for emb_name, feat_names in shared_emb_config.items():\n",
    "                for feat_name in feat_names:\n",
    "                    if feat_name in self._emb_dict:\n",
    "                        self._emb_dict[feat_name] = emb_name\n",
    "\n",
    "        self._seq_names = sorted(list(self._seq_names))\n",
    "\n",
    "        # 初始化嵌入层\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            feat_name: nn.Embedding(config['emb_shape'][0], config['emb_shape'][1])\n",
    "            for feat_name, config in self.input_config.items() if config['category'] == 'embedding'\n",
    "        })\n",
    "\n",
    "        # MLP深度部分\n",
    "        self.dnn = nn.Sequential(\n",
    "            nn.Linear(200, 80),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(80, 2)\n",
    "        )\n",
    "\n",
    "        # Wide部分 - 使用线性层来进行交叉\n",
    "        self.wide = nn.Linear(200, 2, bias=False)\n",
    "\n",
    "    def forward(self, features, mode='train'):\n",
    "        # 分离特征\n",
    "        separated_features = { \n",
    "            'embedding': {}, 'seq': {}, 'mask': {}, 'value': {}\n",
    "        }\n",
    "        for feat_name, feat_values in features.items():\n",
    "            config = self.input_config[feat_name]\n",
    "            category = config['category']\n",
    "            if category == 'embedding':\n",
    "                separated_features['embedding'][feat_name] = feat_values\n",
    "            elif category == 'sequence':\n",
    "                seq_name = config['seq_name']\n",
    "                if seq_name not in separated_features['seq']:\n",
    "                    separated_features['seq'][seq_name] = {}\n",
    "                separated_features['seq'][seq_name][feat_name] = feat_values\n",
    "            elif category == 'mask':\n",
    "                separated_features['mask'][feat_name] = feat_values\n",
    "            elif category == 'value':\n",
    "                separated_features['value'][feat_name] = feat_values\n",
    "\n",
    "        # 获取嵌入\n",
    "        emb_feats = {feat_name: self.embeddings[feat_name](feat_values) for feat_name, feat_values in separated_features['embedding'].items()}\n",
    "        \n",
    "        # 合并向量部分\n",
    "        vec_cat = torch.cat([emb_feats[feat_name] for feat_name in emb_feats], dim=-1)\n",
    "\n",
    "        # 序列部分处理：这里我们做一个简单的sum pool操作\n",
    "        seq_cat = {}\n",
    "        for seq_name, seq_feats in separated_features['seq'].items():\n",
    "            seq_cat[seq_name] = torch.sum(torch.stack([emb_feats[feat_name] for feat_name in seq_feats], dim=1), dim=1)\n",
    "\n",
    "        # Wide部分 - 特征对内积\n",
    "        wide_inputs = [vec_cat]\n",
    "        for field1_idx in range(len(vec_cat) - 1):\n",
    "            field1 = vec_cat[field1_idx]\n",
    "            for field2_idx in range(field1_idx + 1, len(vec_cat)):\n",
    "                field2 = vec_cat[field2_idx]\n",
    "                wide_inputs.append(field1 * field2)\n",
    "\n",
    "        wide_input = torch.cat(wide_inputs, dim=-1)\n",
    "        wide_logits = self.wide(wide_input)\n",
    "\n",
    "        # Deep部分 - 使用MLP\n",
    "        deep_input = torch.cat([vec_cat] + list(seq_cat.values()), dim=-1)\n",
    "        deep_logits = self.dnn(deep_input)\n",
    "\n",
    "        # 输出\n",
    "        logits = wide_logits + deep_logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
